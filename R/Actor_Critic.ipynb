{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0de665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Training Actor-Critic (A2C) for 1000 episodes...\n",
      "-----------------------------------\n",
      "Episode 100: Total Reward = 5\n",
      "Episode 200: Total Reward = 5\n",
      "Episode 300: Total Reward = 5\n",
      "Episode 400: Total Reward = 5\n",
      "Episode 500: Total Reward = 5\n",
      "Episode 600: Total Reward = 5\n",
      "Episode 700: Total Reward = 5\n",
      "Episode 800: Total Reward = 5\n",
      "Episode 900: Total Reward = 5\n",
      "Episode 1000: Total Reward = 5\n",
      "\n",
      "Training Finished.\n",
      "\n",
      "-----------------------------------\n",
      "Testing Learned Policy (Greedy Mode)\n",
      "-----------------------------------\n",
      "Final Path Taken: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (3, 2), (3, 3)]\n",
      "RESULT: SUCCESS - Goal Reached!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.rows = 4\n",
    "        self.cols = 4\n",
    "        self.state = (0, 0)\n",
    "        self.goal = (3, 3)\n",
    "        self.actions = [0, 1, 2, 3] # Up, Right, Down, Left\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:   x = max(0, x - 1)\n",
    "        elif action == 1: y = min(self.cols - 1, y + 1)\n",
    "        elif action == 2: x = min(self.rows - 1, x + 1)\n",
    "        elif action == 3: y = max(0, y - 1)\n",
    "        \n",
    "        self.state = (x, y)\n",
    "        if self.state == self.goal: return self.state, 10, True\n",
    "        else: return self.state, -1, False\n",
    "\n",
    "# --- HELPER FUNCTION FOR ONE-HOT ENCODING ---\n",
    "def state_to_dqn_input(state):\n",
    "    # Converts (row, col) into a one-hot vector of size 16\n",
    "    # Example: (0,0) -> [1, 0, 0, ... 0]\n",
    "    one_hot = np.zeros(16)\n",
    "    index = state[0] * 4 + state[1]\n",
    "    one_hot[index] = 1\n",
    "    return torch.FloatTensor(one_hot)\n",
    "\n",
    "def run_a2c():\n",
    "    env = GridWorld()\n",
    "    \n",
    "    # INPUT SIZE CHANGED TO 16 (4x4 grid flattened)\n",
    "    actor = nn.Sequential(\n",
    "        nn.Linear(16, 128), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(128, 4), \n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "    \n",
    "    critic = nn.Sequential(\n",
    "        nn.Linear(16, 128), \n",
    "        nn.ReLU(), \n",
    "        nn.Linear(128, 1)\n",
    "    )\n",
    "    \n",
    "    opt_a = optim.Adam(actor.parameters(), lr=0.003) # Increased LR slightly\n",
    "    opt_c = optim.Adam(critic.parameters(), lr=0.003)\n",
    "    \n",
    "    episodes = 1000\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Training Actor-Critic (A2C) for {episodes} episodes...\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # FIX: Use One-Hot Encoding\n",
    "            state_t = state_to_dqn_input(state)\n",
    "            \n",
    "            # Actor decides\n",
    "            dist = torch.distributions.Categorical(actor(state_t))\n",
    "            action = dist.sample()\n",
    "            \n",
    "            next_state, reward, done = env.step(action.item())\n",
    "            \n",
    "            # FIX: Use One-Hot Encoding for next state\n",
    "            next_state_t = state_to_dqn_input(next_state)\n",
    "            \n",
    "            # Critic evaluates\n",
    "            val = critic(state_t)\n",
    "            next_val = critic(next_state_t)\n",
    "            \n",
    "            # TD Error (Advantage)\n",
    "            target = reward + 0.99 * next_val.detach() * (not done)\n",
    "            td_error = target - val\n",
    "            \n",
    "            # Updates\n",
    "            a_loss = -dist.log_prob(action) * td_error.detach()\n",
    "            c_loss = td_error.pow(2)\n",
    "            \n",
    "            opt_a.zero_grad(); a_loss.backward(); opt_a.step()\n",
    "            opt_c.zero_grad(); c_loss.backward(); opt_c.step()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if steps > 100: break # Break infinite loops\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    print(\"\\nTraining Finished.\")\n",
    "\n",
    "    # --- TESTING PHASE ---\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(\"Testing Learned Policy (Greedy Mode)\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 20:\n",
    "        state_t = state_to_dqn_input(state)\n",
    "        with torch.no_grad():\n",
    "            probs = actor(state_t)\n",
    "            action = torch.argmax(probs).item()\n",
    "            \n",
    "        state, _, done = env.step(action)\n",
    "        path.append(state)\n",
    "        steps += 1\n",
    "        \n",
    "    print(\"Final Path Taken:\", path)\n",
    "    if path[-1] == (3,3):\n",
    "        print(\"RESULT: SUCCESS - Goal Reached!\")\n",
    "    else:\n",
    "        print(\"RESULT: FAILED - Did not reach goal.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_a2c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f5d0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, mean return -516.81\n",
      "Episode 100, mean return -290.54\n",
      "Episode 150, mean return -248.91\n",
      "Episode 200, mean return -336.02\n",
      "Episode 250, mean return -286.13\n",
      "Episode 300, mean return -436.40\n",
      "Episode 350, mean return -223.14\n",
      "Episode 400, mean return -359.05\n",
      "A2C training completed for Pendulum-v1.\n"
     ]
    }
   ],
   "source": [
    "# exp12_a2c_continuous.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.net(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "def select_action(actor, state):\n",
    "    s = torch.tensor(state, dtype=torch.float32)\n",
    "    mu, std = actor(s)\n",
    "    dist = torch.distributions.Normal(mu, std)\n",
    "    a = dist.sample()\n",
    "    return a.detach().numpy(), dist.log_prob(a).sum()\n",
    "\n",
    "def train_a2c(episodes=400, gamma=0.99, lr=3e-4):\n",
    "    actor = Actor(obs_dim, act_dim)\n",
    "    critic = Critic(obs_dim)\n",
    "    a_opt = optim.Adam(actor.parameters(), lr=lr)\n",
    "    c_opt = optim.Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, logp = select_action(actor, state)\n",
    "            value = critic(torch.tensor(state, dtype=torch.float32))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            log_probs.append(logp)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # compute returns and advantages\n",
    "        R = 0.0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        values = torch.stack(values)\n",
    "        advantages = returns - values.detach()\n",
    "\n",
    "        # actor update\n",
    "        a_opt.zero_grad()\n",
    "        log_probs_t = torch.stack(log_probs)\n",
    "        actor_loss = -(log_probs_t * advantages).mean()\n",
    "        actor_loss.backward()\n",
    "        a_opt.step()\n",
    "\n",
    "        # critic update\n",
    "        c_opt.zero_grad()\n",
    "        critic_loss = nn.MSELoss()(values, returns)\n",
    "        critic_loss.backward()\n",
    "        c_opt.step()\n",
    "\n",
    "        if (ep + 1) % 50 == 0:\n",
    "            print(f\"Episode {ep+1}, mean return {returns.mean().item():.2f}\")\n",
    "\n",
    "    return actor, critic\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actor, critic = train_a2c()\n",
    "    print(\"A2C training completed for Pendulum-v1.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
