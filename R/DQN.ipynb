{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f483f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Starting Training for 500 episodes...\n",
      "-----------------------------------\n",
      "Episode 50: Total Reward = 5, Loss = 2.4373\n",
      "Episode 100: Total Reward = 4, Loss = 3.4601\n",
      "Episode 150: Total Reward = 2, Loss = 12.3485\n",
      "Episode 200: Total Reward = 5, Loss = 0.0437\n",
      "Episode 250: Total Reward = 4, Loss = 1.0288\n",
      "Episode 300: Total Reward = 5, Loss = 0.0090\n",
      "Episode 350: Total Reward = 5, Loss = 0.0048\n",
      "Episode 400: Total Reward = 5, Loss = 0.0025\n",
      "Episode 450: Total Reward = 5, Loss = 0.0012\n",
      "Episode 500: Total Reward = 5, Loss = 0.0153\n",
      "\n",
      "Training Finished.\n",
      "\n",
      "-----------------------------------\n",
      "Testing Learned Policy (Greedy Mode)\n",
      "-----------------------------------\n",
      "Final Path Taken: [(0, 0), (0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (3, 3)]\n",
      "RESULT: SUCCESS - Goal Reached!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.rows = 4\n",
    "        self.cols = 4\n",
    "        self.state = (0, 0)\n",
    "        self.goal = (3, 3)\n",
    "        self.actions = [0, 1, 2, 3] # Up, Right, Down, Left\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:   x = max(0, x - 1)\n",
    "        elif action == 1: y = min(self.cols - 1, y + 1)\n",
    "        elif action == 2: x = min(self.rows - 1, x + 1)\n",
    "        elif action == 3: y = max(0, y - 1)\n",
    "        \n",
    "        self.state = (x, y)\n",
    "        if self.state == self.goal: return self.state, 10, True\n",
    "        else: return self.state, -1, False\n",
    "\n",
    "def run_dqn():\n",
    "    env = GridWorld()\n",
    "    \n",
    "    # Define Model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 4)\n",
    "    )\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    gamma = 0.9\n",
    "    epsilon = 0.3 # Increased slightly for better initial exploration\n",
    "    episodes = 500 # Increased slightly to ensure convergence\n",
    "    \n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Starting Training for {episodes} episodes...\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    # --- TRAINING LOOP ---\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        while not done:\n",
    "            state_t = torch.FloatTensor(state)\n",
    "            \n",
    "            # Epsilon Greedy with decay (optional, but helps)\n",
    "            current_epsilon = max(0.01, epsilon * (0.99 ** episode))\n",
    "            \n",
    "            if random.random() < current_epsilon:\n",
    "                action = random.choice(env.actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state_t)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state_t = torch.FloatTensor(next_state)\n",
    "            \n",
    "            # --- DQN Update ---\n",
    "            # 1. Calculate Target\n",
    "            with torch.no_grad():\n",
    "                target_max = model(next_state_t).max()\n",
    "                target = reward + gamma * target_max * (not done)\n",
    "            \n",
    "            # 2. Calculate Prediction\n",
    "            q_values = model(state_t)\n",
    "            q_pred = q_values[action]\n",
    "            \n",
    "            # 3. Optimize\n",
    "            loss = loss_fn(q_pred, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print progress every 50 episodes\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode+1}: Total Reward = {total_reward}, Loss = {total_loss:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining Finished.\")\n",
    "    \n",
    "    # --- TESTING PHASE ---\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(\"Testing Learned Policy (Greedy Mode)\")\n",
    "    print(\"-----------------------------------\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    path = [state]\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 20:\n",
    "        state_t = torch.FloatTensor(state)\n",
    "        with torch.no_grad():\n",
    "            # Strictly Greedy (No epsilon)\n",
    "            q_values = model(state_t)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "        state, _, done = env.step(action)\n",
    "        path.append(state)\n",
    "        steps += 1\n",
    "        \n",
    "    print(\"Final Path Taken:\", path)\n",
    "    \n",
    "    if path[-1] == (3,3):\n",
    "        print(\"RESULT: SUCCESS - Goal Reached!\")\n",
    "    else:\n",
    "        print(\"RESULT: FAILED - Did not reach goal.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c372261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhijeet P\\AppData\\Local\\Temp\\ipykernel_6788\\3576176886.py:96: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.tensor([one_hot_state(b[0], env.n_states) for b in batch], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, eps=0.778\n",
      "Episode 100, eps=0.606\n",
      "Episode 150, eps=0.471\n",
      "Episode 200, eps=0.367\n",
      "Episode 250, eps=0.286\n",
      "Episode 300, eps=0.222\n",
      "Episode 350, eps=0.173\n",
      "Episode 400, eps=0.135\n",
      "Episode 450, eps=0.105\n",
      "Episode 500, eps=0.082\n",
      "Episode 550, eps=0.063\n",
      "Episode 600, eps=0.050\n",
      "Episode 650, eps=0.050\n",
      "Episode 700, eps=0.050\n",
      "Episode 750, eps=0.050\n",
      "Episode 800, eps=0.050\n",
      "DQN-derived policy (0:U,1:R,2:D,3:L):\n",
      "[[1 2 2 2]\n",
      " [1 1 2 2]\n",
      " [2 1 2 2]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# exp9_dqn_gridworld.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GridWorld4x4:\n",
    "    def __init__(self, start_state=0, goal_state=15, max_steps=100):\n",
    "        self.n_rows = 4\n",
    "        self.n_cols = 4\n",
    "        self.n_states = self.n_rows * self.n_cols\n",
    "        self.n_actions = 4\n",
    "        self.start_state = start_state\n",
    "        self.goal_state = goal_state\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "    def state_to_xy(self, s):\n",
    "        return (s // self.n_cols, s % self.n_cols)\n",
    "\n",
    "    def xy_to_state(self, r, c):\n",
    "        return r * self.n_cols + c\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        self.steps = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        r, c = self.state_to_xy(self.state)\n",
    "        if action == 0:\n",
    "            r = max(0, r - 1)\n",
    "        elif action == 1:\n",
    "            c = min(self.n_cols - 1, c + 1)\n",
    "        elif action == 2:\n",
    "            r = min(self.n_rows - 1, r + 1)\n",
    "        elif action == 3:\n",
    "            c = max(0, c - 1)\n",
    "        ns = self.xy_to_state(r, c)\n",
    "        self.state = ns\n",
    "        self.steps += 1\n",
    "        done = (ns == self.goal_state) or (self.steps >= self.max_steps)\n",
    "        reward = 0 if ns == self.goal_state else -1\n",
    "        return ns, reward, done, {}\n",
    "\n",
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def one_hot_state(s, n_states):\n",
    "    v = np.zeros(n_states, dtype=np.float32)\n",
    "    v[s] = 1.0\n",
    "    return v\n",
    "\n",
    "def train_dqn(env, episodes=1000, gamma=1.0, batch_size=32, lr=1e-3):\n",
    "    device = torch.device(\"cpu\")\n",
    "    net = DQNNet(env.n_states, env.n_actions).to(device)\n",
    "    target_net = DQNNet(env.n_states, env.n_actions).to(device)\n",
    "    target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    replay_buffer = deque(maxlen=5000)\n",
    "\n",
    "    eps = 1.0\n",
    "    eps_end = 0.05\n",
    "    eps_decay = 0.995\n",
    "    update_target_every = 50\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.random() < eps:\n",
    "                a = random.randrange(env.n_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    inp = torch.tensor(one_hot_state(s, env.n_states), dtype=torch.float32)\n",
    "                    q_vals = net(inp)\n",
    "                    a = int(torch.argmax(q_vals).item())\n",
    "            ns, r, done, _ = env.step(a)\n",
    "            replay_buffer.append((s, a, r, ns, done))\n",
    "            s = ns\n",
    "\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = random.sample(replay_buffer, batch_size)\n",
    "                states = torch.tensor([one_hot_state(b[0], env.n_states) for b in batch], dtype=torch.float32)\n",
    "                actions = torch.tensor([b[1] for b in batch], dtype=torch.int64).unsqueeze(1)\n",
    "                rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32)\n",
    "                next_states = torch.tensor([one_hot_state(b[3], env.n_states) for b in batch], dtype=torch.float32)\n",
    "                dones = torch.tensor([b[4] for b in batch], dtype=torch.float32)\n",
    "\n",
    "                q_values = net(states).gather(1, actions).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    next_q_values = target_net(next_states).max(1)[0]\n",
    "                    targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "\n",
    "                loss = nn.MSELoss()(q_values, targets)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        eps = max(eps_end, eps * eps_decay)\n",
    "        if (ep + 1) % update_target_every == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "            print(f\"Episode {ep+1}, eps={eps:.3f}\")\n",
    "\n",
    "    # extract policy\n",
    "    policy = np.zeros(env.n_states, dtype=int)\n",
    "    for s in range(env.n_states):\n",
    "        with torch.no_grad():\n",
    "            inp = torch.tensor(one_hot_state(s, env.n_states), dtype=torch.float32)\n",
    "            q_vals = net(inp)\n",
    "            policy[s] = int(torch.argmax(q_vals).item())\n",
    "    return net, policy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld4x4()\n",
    "    net, pi = train_dqn(env, episodes=800)\n",
    "    print(\"DQN-derived policy (0:U,1:R,2:D,3:L):\")\n",
    "    print(pi.reshape(4, 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
